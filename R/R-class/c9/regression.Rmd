---
title: "Regression"
author: "Elara"
date: "2016年5月4日"
output:
  html_document:
    number_sections: yes
    theme: cerulean
  pdf_document:
    includes:
      in_header: header.tex
    latex_engine: xelatex
  word_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 线性回归       

## 线性模型
$$Y_i=e^{\beta_1+\beta_2X_i+\epsilon_i}$$      
$$Y_i=\frac{1}{e^{\beta_1+\beta_2X_i+\epsilon_i}}$$    
$$Y_i=\beta_1+(0.75-\beta_1)e^{-\beta_2(X_i-2)}+\epsilon_i$$       
$$Y_i=\beta_1+\beta_2^{3}X_i+\epsilon_i$$         
$$Y_i=\beta_1+\beta_2(\frac{1}{X_i})+\epsilon_i$$   


1. 125是线性模型 
2. 没有截距项的时候R2不能用。此时OLS的FOC没有$\beta_0$相关，得不到残差和=0    
3. 无法把方差分解成可解释和不可解释部分。        
4. 即使截距项不显著也不能去掉。去掉的话一定过原点。      
5. R2受到模型变量数目影响。要用adj.R2    

##LM线性模型估计OLS
```{r }
#Y X 线性
options(digits=3)
fit <- lm(weight ~ height, data = women)
summary(fit)

coefficients(fit)

fitted(fit)

residuals(fit)

deviance(fit)
#置信区间0.99
confint(fit,level=0.99)

plot(women$height,women$weight,main="Women Age 30-39",xlab="Height",ylab="Weight")

abline(fit)
```

```{r}
#x和y非线性
fit2 <- lm(weight ~ height + I(height^2), data=women)
summary(fit2)
plot(women$height, women$weight, main = "Women Age 30-39",
xlab = "Height", ylab = "Weight")
lines(women$height, fitted(fit2))

```

```{r}
Anscombe<-data.frame(
X =c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),
Y1=c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),
Y2=c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),
Y3=c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.44, 5.73),
X4=c(rep(8,7), 19, rep(8,3)),
Y4=c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)
)
summary(lm(Y1~X, data=Anscombe))
summary(lm(Y2~X, data=Anscombe))
summary(lm(Y3~X, data=Anscombe))
summary(lm(Y4~X4,data=Anscombe))
head(Anscombe)
attach(Anscombe)
par(mfrow = c(2,2))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y1); abline(lm(Y1~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y2); abline(lm(Y2~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y3); abline(lm(Y3~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X4,Y4); abline(lm(Y4~X4))
```

系数都是3和0.5并且都显著。可是作图结果形状完全不一致2是曲线3有异常值4除了一个点以外都是同一个竖线上

```{r}
#1没有问题
par(mfrow = c(1,1))
#2是个曲线，加入平方拟合
X2<-X^2
#存放用平方拟合的系数
lm2.sol<-lm(Y2~X+X2)
summary(lm2.sol)
#作图用x
x<-seq(min(X), max(X), by=0.1)
#作图用系数
b<-coef(lm2.sol)
y<-b[1]+b[2]*x+b[3]*x^2
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y")
#plot原图
points(X,Y2)
lines(x,y)
#3
#去掉第三个（异常值）
i<-1:11; Y31<-Y3[i!=3]; X3<-X[i!=3]
lm3.sol<-lm(Y31~X3)
summary(lm3.sol)
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y")
points(X,Y3)
abline(lm3.sol)
detach(Anscombe)
```

## 异常值检测

1.diffits指标

$$DFFITS=\frac{\hat{y_i}-\hat{y_{i(i)}}}{s_(i)\sqrt{h_{ii}}}\sqrt{h_{ii}/(1-h_{ii})}$$

h是帽子矩阵，y尖=hy

```{r}
attach(Anscombe)
p<-1; n<-length(X);d<-dffits(lm(Y3~X, data=Anscombe))
cf<-1:n; cf[d>2*sqrt((p+1)/n)]
#取出1到n里面满足dffits大于2根号（（p+1）/n）
detach(Anscombe)
```

返回了异常值位置3

2.Cook's distance

$$D_i=\frac{(\hat{\beta}-\hat{\beta}^{(-i)})^{T}(X^{T}X)(\hat{\beta}-\hat{\beta}^{(-i)})}{(1+p)s^{2}}$$


```{r}
Fit<-lm(Y3~X, data=Anscombe)
cooks.distance(Fit)
par(mfrow=c(2,1))
#散点图
plot(cooks.distance(Fit),main="Cook's distance",cex=0.5)
#线图，红线表示警戒线
Np<-length(coefficients(Fit))-1#变量数
N<-length(fitted(Fit))
#红线算法
CutLevel<-4/(N-Np-1)
plot(Fit,which=4)
abline(CutLevel,0,lty=2,col="red")
```

summary

```{r}
#可以直接算dffit和cook，有问题的会带星号
influence.measures(lm(Y3~X, data=Anscombe))
```


## 最大似然估计
The following function is called a likelihood function, denoted by LF($\beta_1$; $\beta_2$; $\sigma^{2}$)
$$
LF(\beta_1,\beta_2,\sigma^{2})=f(Y_1,Y_2,\dots,Y_n|\beta_1+\beta_2X_i,\sigma^{2})=\frac{1}{\sigma^{n}(\sqrt{2\pi})^{n}}exp(\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}})
$$      
where $\beta_1$; $\beta_2$; $\sigma^{2}$ are not known. The method of maximum likelihood, as the name indicates, consists in estimating the unknown parameters in such a manner that the probability of observing the given Y’s is as high (or maximum) as possible. Therefore, we have to find the maximum of the function 6. For differentiation it is easier to express 6 in the log term as follows:   
$$
ln LF = -nln\sigma -\frac{n}{2}ln(2\pi)-\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}} \\
= -\frac{n}{2}ln\sigma^{2} -\frac{n}{2}ln(2\pi)-\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}}
$$
Differentiating 7 partially with respect to $\beta_1$; $\beta_2$, and $\sigma^{2}$, we can obtain the ML estimators.         

```
install.packages(maxLik)
```     

```{r}
library("maxLik")
indfood<-read.csv(file="C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Indfood.csv")
#抽取数据
foodexp<-indfood[,1]
totalexp<-indfood[,2]
#OLS回归
lm_r <- lm(foodexp~totalexp)
summary(lm_r)
#最大似然估计
#对数似然函数
loglik=function (para){
N=length(foodexp)#样本量
e=foodexp-para[1]-para[2]*totalexp#残差项表达式，para是参数估计量
ll=-0.5*N*log(2*pi)-0.5*N*log(para[3]^2)-0.5*sum(e^2/para[3]^2)#对数似然函数，注意有个参数3
return(ll)
}
#需要1，log后的似然函数，初始值
mle1=maxLik(loglik,start=c(0.1,1,1))#3个参数，β1 β2，方差
coef(mle1)

```

## 多元线性回归

OLS是线性无偏中方差最小的。如果有一个有偏估计方差很小也可以用

```{r}
class(mtcars)
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl",
"disp", "hp", "wt")])
cor(mtcar)
library(car)
scatterplotMatrix(mtcar, spread=FALSE, main="Scatter Plot Matrix")
fit3 <- lm(mpg ~ hp + wt + hp:wt, data = mtcar)
summary(fit3)
fit <- lm(weight ~ height, data=women)
par(mfrow=c(2,2))
plot(fit)

```

残差图如果是左右开口的喇叭状很可能有异方差

第四图：高杠杆有离群点，强影响(红实线是警戒线)

```{r}
#加入平方项回归
fit2 <- lm(weight ~ height + I(height^2), data=women)
plot(fit2)
```

## 系数之间相关影响实验
In order to explain the meaning of coefficients ,we have the following step.    
Regression model:       
$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+u_i$$ 
Step 1: 
$$w_i=y_i-\hat{\alpha}_0-\hat{\alpha}_1x_{i2}$$ 
Step 2: 
$$v_i=x_{i1}-\hat{b}_0-\hat{b}_1x_{i2}$$        
Step 3: 
$$\bar{\beta}_1=\frac{\sum v_iw_i}{\sum v_i^{2}}$$      

```{r}
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl", "disp", "hp", "wt")])
fit <- lm(mpg~wt+disp, data=mtcar)
summary(fit)
fit1 <- lm(mpg~disp, data=mtcar)
fit2 <- lm(wt~disp, data=mtcar)
fit3 <- lm(fit1$residuals~fit2$residuals-1)#没常数项用-1
summary(fit3)
```

说明x2对x1系数没有影响

## 置信区间
```{r}
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl", "disp", "hp", "wt")])
mtcarn<-mtcar[order(mtcar$wt),]
fit <- lm(mpg~wt, data=mtcarn)
conf=predict(fit,interval="confidence",level=0.95)
conf
plot(mpg~wt, data=mtcarn)
abline(fit)
lines(mtcarn$wt,conf[,2],lty=3,col="blue")
lines(mtcarn$wt,conf[,3],lty=3,col="blue")
```

## 假设检验

```{r }
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl",
"disp", "hp", "wt")])
library(car)
fit <- lm(mpg ~ hp + wt, data = mtcar)
summary(fit)
```

```{r}
linearHypothesis(fit, "hp = 0")#变量hp的系数=0

linearHypothesis(fit, "hp = -0.5")

linearHypothesis(fit, "hp - wt= 0")#hp和wt相等
```

F检验的f值，总是对应假设中T检验t值的平方（要在相同原假设下采用正确形式）


## 虚拟变量（Dummy）
例如：是否要加入异常值、季节调整、经济结构改变（名义变量）            
dummy variable 的取值是0和1             
- coincident regression ：2个方程截距和斜率都相等       
- parallel regression：2个方程仅截距项不同      
- concurrent regression：2个方程仅斜率不同      
- dissimilar regression：2个方程斜率和截距项都不同         
![](C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\1.png)      

如果只有一个虚拟变量就直接加入方程回归          
有多个状态（lebvel）的时候要建立多个虚拟变量            
例如3个状态（is Asian？）（is Caucasian？）（is African Americ？）      
需要2个随机变量         
$$
D_{i1} = \left\{
\begin{array}{ll}
1 & \textrm{if ith person is Asian}\\
0 & \textrm{if ith person is not Asian}\\
\end{array} 
\right.
$$         
and     
$$
D_{i2} = \left\{
\begin{array}{ll}
1 & \textrm{if ith person is Caucasian}\\
0 & \textrm{if ith person is not Caucasian}\\
\end{array} 
\right.
$$              
回归模型为：    
$$
y_i=\beta_0+\beta_1D_{i1}+\beta_2D_{i2}+\epsilon_i=
\left\{ 
\begin{array}{ll}
\beta_0+\beta_1+\epsilon_i & \textrm{if ith person is Asian} \\
\beta_0+\beta_2+\epsilon_i & \textrm{if ith person is Caucasian} \\
\beta_0+\epsilon_i & \textrm{if ith person is African Americ} \\
\end{array} 
\right.
$$       
此时$\beta_0$ 是African Americ的基准水平，$\beta_1$ 是Asian和African Americ之间基准水平的差距，$\beta_2$ 是Caucasian和African            Americ之间基准水平的差距                        
其中African Americ叫基准类（baseline：不含$\beta_1$ 和$\beta_2$）               
3个level只要2个虚拟变量，加了3个会有完全共线性                  


```{r}
#导入消费数据
con_China<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\consumption_China.csv")
names(con_China)<-c("year","c","y")
names(con_China)
#判别>=1992为true，小于的false
Dummy<-con_China$year>=1992
Dummy
#改变成0-1
Dummy<-as.numeric(Dummy)
Dummy
#导入工资数据
wage1<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\wage1.csv",header=T)
head(wage1)
#回归0。结果带星号显著
lm.wage<-lm(lwage~ female+educ+exper+expersq+tenure+tenursq,data=wage1)
summary(lm.wage)
#生成4个虚拟变量，结婚|男女
attach(wage1)
marrmale<-as.numeric(married==1 & female==0)
marrfemale<-as.numeric(married==1 & female==1)
singmale<-as.numeric(married==0 & female==0)
singfem<-as.numeric(married==0 & female==1)
#只放3个虚拟变量进入回归1
lm.wage1<-lm(lwage~marrmale+marrfemale+singfem+educ+exper+tenure+expersq+tenursq)
summary(lm.wage1)
#放入全部4个level，最后一个加入的虚拟变量系统默认全为NA无法计算（因为完全多重共线性）
lm.wage2<-lm(lwage~marrmale+marrfemale+singfem+educ+exper+tenure+expersq+tenursq+singmale)
summary(lm.wage2)
#baseline:单身男性，则已婚男性的截距为系数married 0.2126756+截距项0.3213780
fema<-female*married
lm.wage3<-lm(lwage~female+married+fema+educ+exper+tenure+expersq+tenursq)
summary(lm.wage3)
detach(wage1)
```

## chow test
检验2个不同数据集的线性回归模型中的系数是否equal(相当于是否存在结构变化)                   
原假设:不存在结构变化                   
1. 用$n=n_1+n_2$总样本进行回归,得到within group sum of squares   :$S_1$,自由度为$n_1+n_2-k$,看是参数个数(包括截距项)                    
2. 在$n_1$和$n_2$ 2个分样本进行回归得到2个回归方程和2个within group sums of square $S_2+S_3$,自由度$n_1+n_2-2k$                 
3. 
$$
F_{k,n_1+n_2-2k} = \frac{[S_1-(S_2+S_3)]/k}{[(S_2+S_3)/(n_1+n_2-2k)]}
$$                   

```{r}
source("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Chow_test.R")
con_China<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\consumption_China.csv")
#View(con_China)
names(con_China)<-c("year","con","GDP")
attach(con_China)
library("ggplot2")
#分离数据
con1<-con[year<=1991]
con2<-con[year>1991]
GDP1<-GDP[year<=1991]
GDP2<-GDP[year>1991]
#标准化
zGDP1=scale(GDP1, center=TRUE, scale=TRUE)[1:14,1]
zcon1=scale(con1, center=TRUE, scale=TRUE)[1:14,1]
zGDP2=scale(GDP2, center=TRUE, scale=TRUE)[1:15,1]
zcon2=scale(con2, center=TRUE, scale=TRUE)[1:15,1]
#合并
n1 <- length(GDP1)
n2 <- length(GDP2)
y1=rep(0,n1)
y2=rep(1,n2)
dat11<-cbind(zGDP1,zcon1,y1)
dat12<-cbind(zGDP2,zcon2,y2)
data1 <- rbind(dat11,dat12)
data1 <- as.data.frame(data1)
names(data1) <- c("GDP","con","year")
#因子化
data1$year <- factor(data1$year,labels =c("before 1991","after 1991"))
#绘图
p <- ggplot(data = data1,aes(x=data1$con,y=data1$GDP))
p+geom_point(aes(color=data1$year))+stat_smooth(aes(color=data1$year))
#chow检验
dat1<-cbind(GDP1,con1) 
dat2<-cbind(GDP2,con2) 
chow(dat1,dat2) 
detach(con_China) 
#P值很小，拒绝经济结构没有发生改变的原假设
```

## 正态性检验
tips:数据量大的时候$\alpha$ 最好定小一些     
大数据量：120以上，这个时候其实基本上渐进正态了，此时$\alpha$ 定1%比较好


### QQ图,Shapiro-Wilk normality test
quantile-quantile plot:比较排序过的样本数据(纵轴)和排序过的正态数据(横轴),如果满足正态性那么样本数据应该在45°线(qq线)附近
Shapiro-Wilk normality test:原假设为满足正态性
```{r}
# Shapiro-Wilk normality test
y <- rnorm(5000)
#Shapiro-Wilk normality test:p很大,无法拒绝原假设
shapiro.test(y)
#QQ图
qqnorm(y)
#QQ线
qqline(y,lty=2)
#Shapiro-Wilk normality test:p很小,拒绝原假设
shapiro.test(runif(100,min=2,max=4))
```

### 正态性检验函数:residplot(自编) car::qqplot 
residplot():画出拟合模型的studentized residual密度函数(红虚线)与密度直方图(默认nbreaks=10),并画出正态密度函数(蓝实线)用来对比   
qqplot():横轴:排序后的t分布生成数,纵轴:样本studentized residuals,并给出置信区间
```{r}
# qqplot residplot
library(car)
fit <- lm(weight ~ height, data=women)
residplot <- function(fit, nbreaks=10) {
z <- rstudent(fit)
hist(z, breaks=nbreaks, freq=FALSE,
xlab="Studentized Residual",
main="Distribution of Errors")
rug(jitter(z), col="brown")
curve(dnorm(x, mean=mean(z), sd=sd(z)),
add=TRUE, col="blue", lwd=2)
lines(density(z)$x, density(z)$y,
col="red", lwd=2, lty=2)
legend("topright",
legend = c( "Normal Curve", "Kernel Density Curve"),
lty=1:2, col=c("blue","red"), cex=.7) }
par(mfrow=c(1,2))
qqPlot(fit,labels=row.names(women),id.method="identify", simulate=TRUE, main="Q-Q Plot")
residplot(fit)
```

### Jarque–Bera (JB) Test of Normality
- 渐近大样本检验
- 基于OLS residual
- S:skewness of OLS residuals,K:kurtosis of OLS residuals,n:样本容量
- $$JB=n[\frac{S^{2}}{6}+\frac{(K-3)^{2}}{24}]$$        
- 正态分布的S=0,K=3,JB检验相当于对联合假设:S=0,K=3进行检验
- 原假设:residuals服从正态分布,此时JB期望为0
- 原假设下JB统计量渐近服从卡方分布(2自由度)

```{r}
library("ccgarch")
jb.test(fit$residuals)
```

## 非线性回归
### 可对数转换
总共有  
1. 线性-线性模型
2. log-log模型，两边都是对数。系数表示x变化1%，y变化的百分比(弹性)
```{r}
cobb<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Cobb.csv",header=T)
cobb
lny<-log(cobb$Y)
lnX2<-log(cobb$X2)
lnX3<-log(cobb$X3)
summary(lm(lny~lnX2+lnX3))
```

we see that in the Taiwanese agricultural sector for the period 1958–1968 the output elasticities of labor and
capital were 0.07285 and 1.07430, respectively. In other words, over the period of study, holding the capital
input constant, a 1 percent increase in the labor input led on the average to about a 0.07 percent increase in
the output. Similarly, holding the labor input constant, a 1 percent increase in the capital input led on the
average to about a 1.07 percent increase in the output. Adding the two output elasticities, we obtain 1.14,
which gives the value of the returns to scale parameter.        

3. log-线性模型，系数表示x变化1，y变化的百分比
4. 线性-log模型，系数表示x变化1%，y变化0.01xbeta

### 不可对数转换
```{r}
cl<-data.frame(
X=c(rep(2*4:21, c(2, 4, 4, 3, 3, 2, 3, 3, 3, 3, 2,
3, 2, 1, 2, 2, 1, 1))),
Y=c(0.49, 0.49, 0.48, 0.47, 0.48, 0.47, 0.46, 0.46,
0.45, 0.43, 0.45, 0.43, 0.43, 0.44, 0.43, 0.43,
0.46, 0.45, 0.42, 0.42, 0.43, 0.41, 0.41, 0.40,
0.42, 0.40, 0.40, 0.41, 0.40, 0.41, 0.41, 0.40,
0.40, 0.40, 0.38, 0.41, 0.40, 0.40, 0.41, 0.38,
0.40, 0.40, 0.39, 0.39)
)
#nonlinear regression
nls.sol<-nls(Y~a+(0.49-a)*exp(-b*(X-8)), data=cl,
start = list( a= 0.1, b = 0.01 ))
nls.sum<-summary(nls.sol); nls.sum
#plot the fitted line and scatter plot
xfit<-seq(8,44,len=200)
yfit<-predict(nls.sol, data.frame(X=xfit))
plot(cl$X, cl$Y)
lines(xfit,yfit)
```
        
# subset selection（模型选择）
## 多重共线性
### 问题
- x1和x2相关0.8-0.9 高度共线性，x1和x2完全线性关系：完全共线性    
- 共线性导致方差增大,估计量不稳定,导致系数的检验T减少,难以进入拒绝域,变得容易得到接受原假设的结果         
- 2个变量的模型存在共线性则方差   
- $$Var\[b_1\]=\frac{\sigma _^{2}}{(1-r_{12}^{2})\sum _{i=1}^{n}(x_{i1}-\bar{x}_1)^{2}}$$ 
- 后果:剔除不显著但实际上应该保留的变量,估计量不稳定,变量系数浮动很大甚至符号发生变化,对样本和变量变化很敏感,R2很大的时候变量缺都不显著     

### 检验
加入或者去掉变量，看其他变量系数是否有明显变化（特别是正负变化）    

单个变量的VIF大于10或者平均VIF大于2     
- $VIF=\frac{1}{(1-R_{k}^2)}$,其中$R_{k}^2$ 是$x_{k}$ 对其他所有变量回归的$R^2$ 

X'X的特征值(Eigenvalue)检验         
- 存在共线性的时候OLS估计量b的MSE是$\sigma^{2}\sum_{j=1}^{p}\lambda_j^{-1}$ ,无共线性的OLS估计量$\hat{\beta}$的MSE是$p\sigma^{2}$      
- 所以比值$L^{*}=\sum_{j=1}^{p} (1/\lambda_j)/p$就是检验统计量.其中$\lambda$ 是X'X的特征值  

condition number            
- $\kappa =(\textrm{largest eigenvalue}/\textrm{smallest eigenvalue})^{1/2}$   

## 解决方法      
- 改变模型结构        
- 剔除有关变量（基本上不可行）    
- 面板数据           
- 逐步回归            
- 岭回归  

###ridge regression
[ridge regression](file:///C:/Users/44180/Documents/R-in-SOE/R/learning%20out%20side/ridge_regression.html)

没有多重共线性也可以用,MSE在k合适的时候会比OLS更小      
估计量:
$$
\hat{\beta}=(X'X+K)^{-1}X'Y
$$
其中X'X和X'Y的元素有相关性
$$
K=
\left(
\begin{array}{cccc}
k_1 & 0 & \ldots & 0  \\
0 & k_2 & \ldots & 0  \\
\vdots & \vdots & \ddots & \vdots \\
0 & \ldots & \ldots & k_p \\
\end{array}
\right)
$$      
K的元素大小增加,$\hat{\beta}$接近0,其偏差随着$k_i$增加而增加,方差随着$k_i$增加而减少

### example
#### 检验多重共线性     
```{r}
#install.packages("MASS")
library("MASS")
## Warning: package ’MASS’ was built under R version 3.2.5
library("car")
load("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\longley.RData")
lm.normal<-lm(y~.,data=longley)
summary(lm.normal)
#R方非常高而且有变量不显著→有可能有多重共线性
#VIF检验(单个>10准则)
vif(lm.normal)
#VIF检验(平均>20准则)
mean(vif(lm.normal))
#相关系数矩阵(2:6去掉Y数据)
XX<-cor(longley[2:6])
#condition number(>1000准则)
kappa(XX,exact=T)
```

#### 进行岭回归
```{r}
#默认λ为0
lm.r<-lm.ridge(y ~ ., longley)
#lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001))可得到λ从0到1每隔0.001的模型
#windows(7,7)
#岭际图(各个变量系数的t检验值与λ的关系曲线)
plot(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001)))
#最佳模型选择()
select(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.0001)))
```

### Monte Carlo Simulation(about ridge regression)       
```{r}
library("MASS")
#1 生成具有共线性的XY;比较OLS和RIDGE系数
set.seed(1234)
x1 <- rnorm(20)
x2 <- rnorm(20,mean=x1,sd=.01)
y <- rnorm(20,mean=3+x1+x2)
lm(y~x1+x2)$coef
lm.ridge(y~x1+x2,lambda=1)
#2 生成n个具有共线性的数据的函数,返回n行3列,列分别是y x1 x2
my.sample <- function (n=20) {
x <- rnorm(n)
x1 <- x + .1*rnorm(n)
x2 <- x + .1*rnorm(n)
y <- 0 + x1 - x2 + rnorm(n)
cbind(y, x1, x2)
}
n <- 500
#n行3列
r <- matrix(NA, nr=n, nc=3)
s <- matrix(NA, nr=n, nc=3)
corr <- matrix(NA, nr=n, nc=3)
#在r里面放
for (i in 1:n) {
#生成500个数据
m <- my.sample()
#corr第i行里面放第i次的x1和x2的相关系数
corr[i,] <- cor(m[,2],m[,3])
#r第i行放第i次数据的OLS的回归系数,包括截距有3列
r[i,] <- lm(m[,1]~m[,-1])$coef
#s的第i行,第2和3列放λ=0.1的岭回归的系数
s[i,2:3] <- lm.ridge(m[,1]~m[,-1], lambda=.1)$coef
s[i,1] <- mean(m[,1])
}
# 画图
#windows(7,7)
#OLS的截距项密度
plot( density(r[,1]), xlim=c(-3,3),
main="Multicolinearity: high variance")
abline(v=0, lty=3)
#OLS的x1系数密度
lines( density(r[,2]), col="red" )
#RIDGE的x1系数密度
lines( density(s[,2]), col="red", lty=2 )
abline(v=1, col="red", lty=3)
#OLS的x2系数密度
lines( density(r[,3]), col="blue" )
#RIDGE的x2系数密度
lines( density(s[,3]), col="blue", lty=2 )
abline(v=-1, col="blue", lty=3) # We give the mean, to show that it is biased
#求密度函数,数据是x以x为基准
evaluate.density <- function (d,x, eps=1e-6) {
density(d, from=x-eps, to=x+2*eps, n=4)$y[2]
}
x<-mean(r[,2]); points( x, evaluate.density(r[,2],x) )
x<-mean(s[,2]); points( x, evaluate.density(s[,2],x) )
x<-mean(r[,3]); points( x, evaluate.density(r[,3],x) )
x<-mean(s[,3]); points( x, evaluate.density(s[,3],x) )
legend( par("usr")[1], par("usr")[4], yjust=1,
c("intercept", "x1", "x2"),
lwd=1, lty=1,
col=c(par("fg"), "red", "blue"))
legend( par("usr")[2], par("usr")[4], yjust=1, xjust=1,
c("classical regression", "ridge regression"),
lwd=1, lty=c(1,2),
col=par("fg") )


apply(r,2,mean)
apply(r,2,var)
apply(s,2,mean)
apply(s,2,var)
n <- 500
v <- matrix(c(0,1,-1), nr=n, nc=3, byrow=T)
mse <- NULL
kl <- c(1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.2, 1.4, 1.6, 1.8, 2.0)
for (k in kl) {
r <- matrix(NA, nr=n, nc=3)
for (i in 1:n) {
m <- my.sample()
r[i,2:3] <- lm.ridge(m[,1]~m[,-1], lambda=k)$coef
r[i,1] <- mean(m[,1])
}
mse <- append(mse, apply( (r-v)^2, 2, mean )[2])
}
plot( mse ~ kl, type="l" )
title(main="MSE evolution")
```

# Best Subset Selection         
k个变量中选出i个变量做回归,并找出最好的i个变量
i从1到
每个模型都是选出i个变量的时候,Cki个模型中RSS最小的模型
```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
#去掉缺失值
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))

library(leaps)
#salary做y,其他做x,逐步加入,分别回归19个模型出来
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
#返回的项目
names(reg.summary)
#每个模型的adjr2,cp,bic
reg.summary$adjr2
reg.summary$cp
reg.summary$bic
#windows(7,7)
par(mfrow=c(2,2))
#RSS与变量个数关系
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
#adjr2与变量个数的关系
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
#找出adjr2最大的模型
which.max(reg.summary$adjr2)
#在图上红色标注
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)
#cp值与变量个数关系
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
#找出cp值最小的模型
which.min(reg.summary$cp)
#标点
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
#找出bic最小
which.min(reg.summary$bic)
#bic与变量个数关系
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
#标点
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
par(mfrow=c(2,2))
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,6)
```

# Stepwise逐步回归
- 向前
- 向后
- 混合
```{r}
cement<-data.frame( X1=c( 7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
X2=c(26, 29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
X3=c( 6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
X4=c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12),
Y =c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1,115.9, 83.8, 113.3, 109.4)
)
lm.sol<-lm(Y ~ X1+X2+X3+X4, data=cement)
summary(lm.sol)
#全部不显著但是r2非常大,有共线性
#逐步删去,按照AIC准则,每行代表去掉这个变量的数据,none代表不去掉,当none的AIC最小时停止
lm.step<-step(lm.sol)
summary(lm.step)
#?
drop1(lm.step)
#最终
lm.opt<-lm(Y ~ X1+X2, data=cement); summary(lm.opt)
```

## 更新模型
```{r}
x1=rnorm(100)
x2=rnorm(100)
x3=rnorm(100)
y=rnorm(100)
fm2 <- lm(y ~ x1 + x2)
#加入x3变量
fm3 <- update(fm2, . ~ . + x3)
#加入y2项
smf3 <- update(fm3, I(y^2) ~ .) 
```