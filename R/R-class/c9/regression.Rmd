---
title: "Regression"
author: "Elara"
date: "2016年5月4日"
output:
  html_document:
    number_sections: yes
    theme: cerulean
  pdf_document:
    includes:
      in_header: header.tex
    latex_engine: xelatex
  word_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 线性回归       

## 线性模型
$$Y_i=e^{\beta_1+\beta_2X_i+\epsilon_i}$$      
$$Y_i=\frac{1}{e^{\beta_1+\beta_2X_i+\epsilon_i}}$$    
$$Y_i=\beta_1+(0.75-\beta_1)e^{-\beta_2(X_i-2)}+\epsilon_i$$       
$$Y_i=\beta_1+\beta_2^{3}X_i+\epsilon_i$$         
$$Y_i=\beta_1+\beta_2(\frac{1}{X_i})+\epsilon_i$$   


1. 125是线性模型 
2. 没有截距项的时候R2不能用。此时OLS的FOC没有$\beta_0$相关，得不到残差和=0    
3. 无法把方差分解成可解释和不可解释部分。        
4. 即使截距项不显著也不能去掉。去掉的话一定过原点。      
5. R2受到模型变量数目影响。要用adj.R2    

##LM线性模型估计OLS
```{r }
#Y X 线性
options(digits=3)
fit <- lm(weight ~ height, data = women)
summary(fit)

coefficients(fit)

fitted(fit)

residuals(fit)

deviance(fit)
#置信区间0.99
confint(fit,level=0.99)

plot(women$height,women$weight,main="Women Age 30-39",xlab="Height",ylab="Weight")

abline(fit)
```

```{r}
#x和y非线性
fit2 <- lm(weight ~ height + I(height^2), data=women)
summary(fit2)
plot(women$height, women$weight, main = "Women Age 30-39",
xlab = "Height", ylab = "Weight")
lines(women$height, fitted(fit2))

```

```{r}
Anscombe<-data.frame(
X =c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),
Y1=c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),
Y2=c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),
Y3=c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.44, 5.73),
X4=c(rep(8,7), 19, rep(8,3)),
Y4=c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)
)
summary(lm(Y1~X, data=Anscombe))
summary(lm(Y2~X, data=Anscombe))
summary(lm(Y3~X, data=Anscombe))
summary(lm(Y4~X4,data=Anscombe))
head(Anscombe)
attach(Anscombe)
par(mfrow = c(2,2))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y1); abline(lm(Y1~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y2); abline(lm(Y2~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X,Y3); abline(lm(Y3~X))
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y"); points(X4,Y4); abline(lm(Y4~X4))
```

系数都是3和0.5并且都显著。可是作图结果形状完全不一致2是曲线3有异常值4除了一个点以外都是同一个竖线上

```{r}
#1没有问题
par(mfrow = c(1,1))
#2是个曲线，加入平方拟合
X2<-X^2
#存放用平方拟合的系数
lm2.sol<-lm(Y2~X+X2)
summary(lm2.sol)
#作图用x
x<-seq(min(X), max(X), by=0.1)
#作图用系数
b<-coef(lm2.sol)
y<-b[1]+b[2]*x+b[3]*x^2
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y")
#plot原图
points(X,Y2)
lines(x,y)
#3
#去掉第三个（异常值）
i<-1:11; Y31<-Y3[i!=3]; X3<-X[i!=3]
lm3.sol<-lm(Y31~X3)
summary(lm3.sol)
plot(c(3,20), c(3,13), type="n", xlab = "X", ylab = "Y")
points(X,Y3)
abline(lm3.sol)
detach(Anscombe)
```

## 异常值检测

1.diffits指标

$$DFFITS=\frac{\hat{y_i}-\hat{y_{i(i)}}}{s_(i)\sqrt{h_{ii}}}\sqrt{h_{ii}/(1-h_{ii})}$$

h是帽子矩阵，y尖=hy

```{r}
attach(Anscombe)
p<-1; n<-length(X);d<-dffits(lm(Y3~X, data=Anscombe))
cf<-1:n; cf[d>2*sqrt((p+1)/n)]
#取出1到n里面满足dffits大于2根号（（p+1）/n）
detach(Anscombe)
```

返回了异常值位置3

2.Cook's distance

$$D_i=\frac{(\hat{\beta}-\hat{\beta}^{(-i)})^{T}(X^{T}X)(\hat{\beta}-\hat{\beta}^{(-i)})}{(1+p)s^{2}}$$


```{r}
Fit<-lm(Y3~X, data=Anscombe)
cooks.distance(Fit)
par(mfrow=c(2,1))
#散点图
plot(cooks.distance(Fit),main="Cook's distance",cex=0.5)
#线图，红线表示警戒线
Np<-length(coefficients(Fit))-1#变量数
N<-length(fitted(Fit))
#红线算法
CutLevel<-4/(N-Np-1)
plot(Fit,which=4)
abline(CutLevel,0,lty=2,col="red")
```

summary

```{r}
#可以直接算dffit和cook，有问题的会带星号
influence.measures(lm(Y3~X, data=Anscombe))
```


## 最大似然估计
The following function is called a likelihood function, denoted by LF($\beta_1$; $\beta_2$; $\sigma^{2}$)
$$
LF(\beta_1,\beta_2,\sigma^{2})=f(Y_1,Y_2,\dots,Y_n|\beta_1+\beta_2X_i,\sigma^{2})=\frac{1}{\sigma^{n}(\sqrt{2\pi})^{n}}exp(\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}})
$$      
where $\beta_1$; $\beta_2$; $\sigma^{2}$ are not known. The method of maximum likelihood, as the name indicates, consists in estimating the unknown parameters in such a manner that the probability of observing the given Y’s is as high (or maximum) as possible. Therefore, we have to find the maximum of the function 6. For differentiation it is easier to express 6 in the log term as follows:   
$$
ln LF = -nln\sigma -\frac{n}{2}ln(2\pi)-\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}} \\
= -\frac{n}{2}ln\sigma^{2} -\frac{n}{2}ln(2\pi)-\frac{1}{2}\sum \frac{(Y_i-\beta_1-\beta_2X_i)^{2}}{\sigma^{2}}
$$
Differentiating 7 partially with respect to $\beta_1$; $\beta_2$, and $\sigma^{2}$, we can obtain the ML estimators.         

```
install.packages(maxLik)
```     

```{r}
library("maxLik")
indfood<-read.csv(file="C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Indfood.csv")
#抽取数据
foodexp<-indfood[,1]
totalexp<-indfood[,2]
#OLS回归
lm_r <- lm(foodexp~totalexp)
summary(lm_r)
#最大似然估计
#对数似然函数
loglik=function (para){
N=length(foodexp)#样本量
e=foodexp-para[1]-para[2]*totalexp#残差项表达式，para是参数估计量
ll=-0.5*N*log(2*pi)-0.5*N*log(para[3]^2)-0.5*sum(e^2/para[3]^2)#对数似然函数，注意有个参数3
return(ll)
}
#需要1，log后的似然函数，初始值
mle1=maxLik(loglik,start=c(0.1,1,1))#3个参数，β1 β2，方差
coef(mle1)

```

## 多元线性回归

OLS是线性无偏中方差最小的。如果有一个有偏估计方差很小也可以用

```{r}
class(mtcars)
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl",
"disp", "hp", "wt")])
cor(mtcar)
library(car)
scatterplotMatrix(mtcar, spread=FALSE, main="Scatter Plot Matrix")
fit3 <- lm(mpg ~ hp + wt + hp:wt, data = mtcar)
summary(fit3)
fit <- lm(weight ~ height, data=women)
par(mfrow=c(2,2))
plot(fit)

```

残差图如果是左右开口的喇叭状很可能有异方差

第四图：高杠杆有离群点，强影响(红实线是警戒线)

```{r}
#加入平方项回归
fit2 <- lm(weight ~ height + I(height^2), data=women)
plot(fit2)
```

## 系数之间相关影响实验
In order to explain the meaning of coefficients ,we have the following step.    
Regression model:       
$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+u_i$$ 
Step 1: 
$$w_i=y_i-\hat{\alpha}_0-\hat{\alpha}_1x_{i2}$$ 
Step 2: 
$$v_i=x_{i1}-\hat{b}_0-\hat{b}_1x_{i2}$$        
Step 3: 
$$\bar{\beta}_1=\frac{\sum v_iw_i}{\sum v_i^{2}}$$      

```{r}
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl", "disp", "hp", "wt")])
fit <- lm(mpg~wt+disp, data=mtcar)
summary(fit)
fit1 <- lm(mpg~disp, data=mtcar)
fit2 <- lm(wt~disp, data=mtcar)
fit3 <- lm(fit1$residuals~fit2$residuals-1)#没常数项用-1
summary(fit3)
```

说明x2对x1系数没有影响

## 置信区间
```{r}
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl", "disp", "hp", "wt")])
mtcarn<-mtcar[order(mtcar$wt),]
fit <- lm(mpg~wt, data=mtcarn)
conf=predict(fit,interval="confidence",level=0.95)
conf
plot(mpg~wt, data=mtcarn)
abline(fit)
lines(mtcarn$wt,conf[,2],lty=3,col="blue")
lines(mtcarn$wt,conf[,3],lty=3,col="blue")
```

## 假设检验

```{r }
mtcar <- as.data.frame(mtcars[,c("mpg", "cyl",
"disp", "hp", "wt")])
library(car)
fit <- lm(mpg ~ hp + wt, data = mtcar)
summary(fit)
```

```{r}
linearHypothesis(fit, "hp = 0")#变量hp的系数=0

linearHypothesis(fit, "hp = -0.5")

linearHypothesis(fit, "hp - wt= 0")#hp和wt相等
```

F检验的f值，总是对应假设中T检验t值的平方（要在相同原假设下采用正确形式）


## 虚拟变量（Dummy）
例如：是否要加入异常值、季节调整、经济结构改变（名义变量）            
dummy variable 的取值是0和1             
- coincident regression ：2个方程截距和斜率都相等       
- parallel regression：2个方程仅截距项不同      
- concurrent regression：2个方程仅斜率不同      
- dissimilar regression：2个方程斜率和截距项都不同         

如果只有一个虚拟变量就直接加入方程回归          
有多个状态（lebvel）的时候要建立多个虚拟变量            
例如3个状态（is Asian？）（is Caucasian？）（is African Americ？）      
需要2个随机变量         
$$
D_{i1} = \left\{
\begin{array}{ll}
1 & \textrm{if ith person is Asian}\\
0 & \textrm{if ith person is not Asian}\\
\end{array} 
\right.
$$         
and     
$$
D_{i2} = \left\{
\begin{array}{ll}
1 & \textrm{if ith person is Caucasian}\\
0 & \textrm{if ith person is not Caucasian}\\
\end{array} 
\right.
$$              
回归模型为：    
$$
y_i=\beta_0+\beta_1D_{i1}+\beta_2D_{i2}+\epsilon_i=
\left\{ 
\begin{array}{ll}
\beta_0+\beta_1+\epsilon_i & \textrm{if ith person is Asian} \\
\beta_0+\beta_2+\epsilon_i & \textrm{if ith person is Caucasian} \\
\beta_0+\epsilon_i & \textrm{if ith person is African Americ} \\
\end{array} 
\right.
$$       
此时$\beta_0$ 是African Americ的基准水平，$\beta_1$ 是Asian和African Americ之间基准水平的差距，$\beta_2$ 是Caucasian和African            Americ之间基准水平的差距                        
其中African Americ叫基准类（baseline：不含$\beta_1$ 和$\beta_2$）               
3个level只要2个虚拟变量，加了3个会有完全共线性                  


```{r}
#导入消费数据
con_China<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\consumption_China.csv")
names(con_China)<-c("year","c","y")
names(con_China)
#判别>=1992为true，小于的false
Dummy<-con_China$year>=1992
Dummy
#改变成0-1
Dummy<-as.numeric(Dummy)
Dummy
#导入工资数据
wage1<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\wage1.csv",header=T)
head(wage1)
#回归0。结果带星号显著
lm.wage<-lm(lwage~ female+educ+exper+expersq+tenure+tenursq,data=wage1)
summary(lm.wage)
#生成4个虚拟变量，结婚|男女
attach(wage1)
marrmale<-as.numeric(married==1 & female==0)
marrfemale<-as.numeric(married==1 & female==1)
singmale<-as.numeric(married==0 & female==0)
singfem<-as.numeric(married==0 & female==1)
#只放3个虚拟变量进入回归1
lm.wage1<-lm(lwage~marrmale+marrfemale+singfem+educ+exper+tenure+expersq+tenursq)
summary(lm.wage1)
#放入全部4个level，最后一个加入的虚拟变量系统默认全为NA无法计算（因为完全多重共线性）
lm.wage2<-lm(lwage~marrmale+marrfemale+singfem+educ+exper+tenure+expersq+tenursq+singmale)
summary(lm.wage2)
#baseline:单身男性，则已婚男性的截距为系数married 0.2126756+截距项0.3213780
fema<-female*married
lm.wage3<-lm(lwage~female+married+fema+educ+exper+tenure+expersq+tenursq)
summary(lm.wage3)
#画图？
detach(wage1)
```

## chow test
检验2个不同数据集的线性回归模型中的系数是否equal(相当于是否存在结构变化)                   
原假设:不存在结构变化                   
1. 用$n=n_1+n_2$总样本进行回归,得到within group sum of squares   :$S_1$,自由度为$n_1+n_2-k$,看是参数个数(包括截距项)                    
2. 在$n_1$和$n_2$ 2个分样本进行回归得到2个回归方程和2个within group sums of square $S_2+S_3$,自由度$n_1+n_2-2k$                 
3. 
$$
F_{k,n_1+n_2-2k} = \frac{[S_1-(S_2+S_3)]/k}{[(S_2+S_3)/(n_1+n_2-2k)]}
$$                   

```{r}
source("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Chow_test.R")
con_China<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\consumption_China.csv")
#View(con_China)
names(con_China)<-c("year","con","GDP")
attach(con_China)
con1<-con[year<=1991]
con2<-con[year>1991]
GDP1<-GDP[year<=1991]
GDP2<-GDP[year>1991]
dat1<-cbind(GDP1,con1)
dat2<-cbind(GDP2,con2)
chow(dat1,dat2)
#P值很小，拒绝经济结构没有发生改变的原假设
```

## 正态性检验
tips:数据量大的时候$\alpha$ 最好定小一些     
大数据量：120以上，这个时候其实基本上渐进正态了，此时$\alpha$ 定1%比较好


### QQ图,Shapiro-Wilk normality test
quantile-quantile plot:比较排序过的样本数据(纵轴)和排序过的正态数据(横轴),如果满足正态性那么样本数据应该在45°线(qq线)附近
Shapiro-Wilk normality test:原假设为满足正态性
```{r}
# Shapiro-Wilk normality test
y <- rnorm(5000)
#Shapiro-Wilk normality test:p很大,无法拒绝原假设
shapiro.test(y)
#QQ图
qqnorm(y)
#QQ线
qqline(y,lty=2)
#Shapiro-Wilk normality test:p很小,拒绝原假设
shapiro.test(runif(100,min=2,max=4))
```

### 正态性检验函数:residplot(自编) car::qqplot 
residplot():画出拟合模型的studentized residual密度函数(红虚线)与密度直方图(默认nbreaks=10),并画出正态密度函数(蓝实线)用来对比   
qqplot():横轴:排序后的t分布生成数,纵轴:样本studentized residuals,并给出置信区间
```{r}
# qqplot residplot
library(car)
fit <- lm(weight ~ height, data=women)
residplot <- function(fit, nbreaks=10) {
z <- rstudent(fit)
hist(z, breaks=nbreaks, freq=FALSE,
xlab="Studentized Residual",
main="Distribution of Errors")
rug(jitter(z), col="brown")
curve(dnorm(x, mean=mean(z), sd=sd(z)),
add=TRUE, col="blue", lwd=2)
lines(density(z)$x, density(z)$y,
col="red", lwd=2, lty=2)
legend("topright",
legend = c( "Normal Curve", "Kernel Density Curve"),
lty=1:2, col=c("blue","red"), cex=.7) }
par(mfrow=c(1,2))
qqPlot(fit,labels=row.names(women),id.method="identify", simulate=TRUE, main="Q-Q Plot")
residplot(fit)
```

### Jarque–Bera (JB) Test of Normality
- 渐近大样本检验
- 基于OLS residual
- S:skewness of OLS residuals,K:kurtosis of OLS residuals,n:样本容量
- $$JB=n[\frac{S^{2}}{6}+\frac{(K-3)^{2}}{24}]$$        
- 正态分布的S=0,K=3,JB检验相当于对联合假设:S=0,K=3进行检验
- 原假设:residuals服从正态分布,此时JB期望为0
- 原假设下JB统计量渐近服从卡方分布(2自由度)

```{r}
library("ccgarch")
jb.test(fit$residuals)
```

## 非线性回归
### 可对数转换
总共有  
1. 线性-线性模型
2. log-log模型，两边都是对数。系数表示x变化1%，y变化的百分比(弹性)
```{r}
cobb<-read.csv("C:\\Users\\44180\\Documents\\R-in-SOE\\R\\R-class\\c9\\Cobb.csv",header=T)
cobb
lny<-log(cobb$Y)
lnX2<-log(cobb$X2)
lnX3<-log(cobb$X3)
summary(lm(lny~lnX2+lnX3))
```

we see that in the Taiwanese agricultural sector for the period 1958–1968 the output elasticities of labor and
capital were 0.07285 and 1.07430, respectively. In other words, over the period of study, holding the capital
input constant, a 1 percent increase in the labor input led on the average to about a 0.07 percent increase in
the output. Similarly, holding the labor input constant, a 1 percent increase in the capital input led on the
average to about a 1.07 percent increase in the output. Adding the two output elasticities, we obtain 1.14,
which gives the value of the returns to scale parameter.        

3. log-线性模型，系数表示x变化1，y变化的百分比
4. 线性-log模型，系数表示x变化1%，y变化0.01xbeta

### 不可对数转换
```{r}
cl<-data.frame(
X=c(rep(2*4:21, c(2, 4, 4, 3, 3, 2, 3, 3, 3, 3, 2,
3, 2, 1, 2, 2, 1, 1))),
Y=c(0.49, 0.49, 0.48, 0.47, 0.48, 0.47, 0.46, 0.46,
0.45, 0.43, 0.45, 0.43, 0.43, 0.44, 0.43, 0.43,
0.46, 0.45, 0.42, 0.42, 0.43, 0.41, 0.41, 0.40,
0.42, 0.40, 0.40, 0.41, 0.40, 0.41, 0.41, 0.40,
0.40, 0.40, 0.38, 0.41, 0.40, 0.40, 0.41, 0.38,
0.40, 0.40, 0.39, 0.39)
)
#nonlinear regression
nls.sol<-nls(Y~a+(0.49-a)*exp(-b*(X-8)), data=cl,
start = list( a= 0.1, b = 0.01 ))
nls.sum<-summary(nls.sol); nls.sum
#plot the fitted line and scatter plot
xfit<-seq(8,44,len=200)
yfit<-predict(nls.sol, data.frame(X=xfit))
plot(cl$X, cl$Y)
lines(xfit,yfit)
```
        
# subset selection（模型选择）
## 多重共线性
### 问题
x1和x2相关0.8-0.9 高度共线性，x1和x2完全线性关系：完全共线性    
理论p42 
方差增大，导致系数的检验T减少，难以进入拒绝域，变得容易得到接受原假设的结果
### 检验
加入或者去掉，看其他系数是否有明显变化（特别是正负变化）
单个变量VIF,大于10或者平均vif大于2 有共线性，
特征值检验      
condition number        
## 解决方法
- 改变模型结构    
- 剔除有关变量（基本上不可行）
- 面板数据        
- 逐步回归        
- 岭回归  


